{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J19_WThCOFGX",
        "outputId": "05e3a1e3-38c0-48cb-ca23-8c5f4ad2e1b6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading text files from the folder\n",
        "def load_text_files(folder_path):\n",
        "    data = []\n",
        "    doc_id_to_filename = {}\n",
        "    for i, filename in enumerate(os.listdir(folder_path)):\n",
        "        if filename.endswith('.txt'):  # Ensure it's a text file\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data.append(file.read())\n",
        "                doc_id_to_filename[i] = filename\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "wKNkWTM6dato"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path\n",
        "folder_path = '/content/drive/MyDrive/documents'"
      ],
      "metadata": {
        "id": "wvdV9NRYeIP9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "docs, doc_id_to_filename = load_text_files(folder_path)"
      ],
      "metadata": {
        "id": "epMy8gW5eVvp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  list of queries\n",
        "queries = [\n",
        "'smart home devices disconnecting network troubleshooting',\n",
        "'MacBook performance slow overheating after software update',\n",
        "'smart TV Wi-Fi disconnecting streaming issue recent update',\n",
        "]"
      ],
      "metadata": {
        "id": "vzWKa9j3eYX6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Preprocessing documents and queries: lowercase and tokenize\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "tokenized_docs = [tokenize(doc) for doc in docs]\n",
        "tokenized_queries = [tokenize(query) for query in queries]"
      ],
      "metadata": {
        "id": "hQZHY3Lkeepg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building vocabulary (unique words across all documents and queries)\n",
        "vocab = set([word for doc in tokenized_docs for word in doc])\n",
        "vocab = sorted(vocab) # Optional sorting for consistency\n",
        "print(\"Vocabulary:\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPEuc2bUeg6R",
        "outputId": "5a78c0f5-8568-4baf-cac7-69eca95bc338"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['5', 'a', 'ability', 'about', 'accurately', 'activity', 'adjusting', 'advice', 'advise', 'affecting', 'afford', 'after', 'ago,', 'almost', 'already', 'also', 'an', 'and', 'android,', 'answer', 'any', 'anything', 'app', 'app,', 'appreciate', 'apps', 'apps,', 'are', 'arrive', 'arrive.', 'arrived.', 'as', 'at', 'back', 'background', 'barely', 'basic', 'battery', 'be', 'be.', 'because', 'becoming', 'been', 'before,', 'between', 'browser', 'business', 'but', 'can', 'can’t', 'caused', 'causing', 'checked', 'common', 'company,', 'compared', 'concrete', 'confident', 'connection', 'consider', 'console', 'constant', 'contacted', 'contacting', 'control', 'correctly', 'could', 'customer', 'date.', 'day', 'day,', 'day.', 'days.', 'delay', 'delivery', 'device', 'device?', 'devices', 'difficult', 'directed', 'directly', 'disconnecting', 'disconnects', 'disconnects?', 'doesn’t', 'don’t', 'dropping', 'each', 'eating', 'else', 'email.', 'end.', 'entirely.', 'especially', 'even', 'ever', 'every', 'everything', 'expect', 'experience.', 'facing', 'factory', 'failing', 'few', 'figure', 'fine', 'firmware', 'fitness', 'fix', 'for', 'freelance', 'frequent', 'from', 'frustrated', 'frustrating', 'frustrating,', 'frustrating.', 'full', 'further', 'gaming', 'gets', 'getting', 'gift,', 'going', 'happening', 'has', 'hasn’t', 'have', 'haven’t', 'having', 'heart', 'help', 'home', 'home.', 'hours,', 'how', 'hub,', 'i', 'idea', 'if', 'important', 'impossible', 'improved', 'improved.', 'in', 'inaccuracy', 'inconvenience.', 'incredibly', 'individually,', 'information', 'installing', 'internet', 'interruptions.', 'into', 'investigate', 'is', 'issue', 'issue.', 'issues.', 'issues?', 'it', 'it,', 'it.”', 'it?', 'it’s', 'i’d', 'i’m', 'i’ve', 'keep', 'keeps', 'know', 'laptop', 'laptop.', 'last', 'latest', 'least', 'let', 'life', 'life.', 'like', 'link', 'look', 'looks', 'lost', 'lost.', 'macbook,', 'major', 'making', 'malware,', 'manage', 'manually', 'me', 'messaging', 'middle', 'might', 'monitor,', 'month', 'more', 'most', 'much', 'multiple', 'my', 'need', 'network', 'new', 'no', 'not', 'nothing', 'noticed', 'now', 'now,', 'of', 'off', 'offline', 'on', 'only', 'optimization', 'or', 'order', 'order?', 'ordered', 'other', 'out', 'over', 'overheating', 'package', 'past', 'performance', 'permanently', 'persists.', 'phone', 'phone,', 'please', 'pram,', 'problem', 'problem,', 'processor.', 'programs', 'progress.', 'provide', 'provided', 'provider,', 'quickly.', 'random', 'randomly', 'rate.', 'readings', 'really', 'rebooted', 'rebooting', 'receive', 'recent', 'reconnect', 'reconnecting', 'recurring', 'refund', 'rely', 'remotely,', 'replacing', 'reported', 'reset,', 'resetting', 'resources.', 'restarting', 'router,', 'router.', 'ruins', 'running', 'runs', 'say', 'scanned', 'security.', 'seem', 'seems', 'seller.', 'serious', 'service', 'services.', 'settings,', 'shipped,', 'shipping', 'should', 'show', 'shows', 'shutdowns?', 'since', 'slow', 'slower', 'smart', 'smc', 'so', 'software', 'solution', 'someone', 'something', 'started', 'steps', 'still', 'stop', 'stream', 'streaming', 'streaming,', 'stuck', 'suggestions', 'supposed', 'sure', 'sync', 'system', 'terrible.', 'than', 'that', 'that’s', 'the', 'their', 'them', 'there', 'there’s', 'these', 'they', 'they’re', 'this', 'this,', 'this.', 'through', 'throughout', 'time,', 'time.', 'times', 'times,', 'to', 'told', 'track', 'tracker', 'tracking', 'transit', 'tried', 'trouble', 'troubleshoot', 'troubleshooting', 'try,', 'turning', 'tv', 'tv,', 'two', 'unable', 'unusual', 'up', 'update', 'update,', 'update.', 'update?', 'updated', 'updates', 'updates,', 'urgently,', 'use', 'used', 'users', 'using', 'version', 'videos', 'viewing', 'warning,', 'was', 'way', 'way?', 'website', 'weeks', 'weeks,', 'what', 'what’s', 'when', 'where', 'whether', 'which', 'wi-fi', 'will', 'windows', 'with', 'within', 'without', 'word', 'work', 'work,', 'work.', 'workouts,', 'worn', 'wrist,', 'wrong', 'yet?', 'you', 'your', '“looking', '\\ufeff@105870', '\\ufeff@105871', '\\ufeff@105872', '\\ufeff@105873', '\\ufeff@105875', '\\ufeff@105876', '\\ufeff@105877', '\\ufeff@105878', '\\ufeff@105879']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate term frequency (TF)\n",
        "def term_frequency(term, document):\n",
        "  return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "Uc7tVCRCejGc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate inverse document frequency (IDF)\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "  num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "  return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "93BCz_OuelD_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing TF-IDF for a document\n",
        "def compute_tfidf(document, all_documents, vocab):\n",
        "  tfidf_vector = []\n",
        "  for term in vocab:\n",
        "    tf = term_frequency(term, document)\n",
        "    idf = inverse_document_frequency(term, all_documents)\n",
        "    tfidf_vector.append(tf * idf)\n",
        "  return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "MMBqxp_CenCo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  norm_vec1 = np.linalg.norm(vec1)\n",
        "  norm_vec2 = np.linalg.norm(vec2)\n",
        "  return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "metadata": {
        "id": "DQOzLm5PepLO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating TF-IDF vectors for documents and queries\n",
        "doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]"
      ],
      "metadata": {
        "id": "vEi4yV7aerXm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path for the output file\n",
        "output_file_path = \"/content/cosine_similarities_output.txt\"\n",
        "\n",
        "# Opening the file in write mode\n",
        "with open(output_file_path, 'w') as f:\n",
        "    # Calculate cosine similarities\n",
        "    cosine_similarities = []\n",
        "    for query_vector in query_tfidf_vectors:\n",
        "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
        "        cosine_similarities.append(similarities)\n",
        "\n",
        "    # Displaying the results in ascending order of cosine similarity\n",
        "    for i, query in enumerate(queries):\n",
        "        f.write(f\"\\nCosine similarities for query '{query}':\\n\")\n",
        "\n",
        "        # Zipping document indices and their corresponding similarities\n",
        "        doc_sim_pairs = list(enumerate(cosine_similarities[i]))\n",
        "\n",
        "        # Sorting the pairs based on similarity in ascending order\n",
        "        doc_sim_pairs_sorted = sorted(doc_sim_pairs, key=lambda x: x[1])\n",
        "\n",
        "        # Writing the sorted document similarities to the file\n",
        "        for doc_idx, similarity in doc_sim_pairs_sorted:\n",
        "            f.write(f\"Document {doc_idx + 1}: {similarity:.4f}\\n\")\n",
        "\n",
        "# Confirming that the output has been saved\n",
        "print(f\"Output has been saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KssycEtflq4",
        "outputId": "7ac8c9f5-b8be-4bba-8360-6c9f0f3f545b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output has been saved to /content/cosine_similarities_output.txt\n"
          ]
        }
      ]
    }
  ]
}